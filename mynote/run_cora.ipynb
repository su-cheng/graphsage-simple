{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2010f131-c074-47a3-8420-7a64bad9c3ca",
   "metadata": {},
   "source": [
    "#### 基本语法\n",
    "##### 随机数生成器\n",
    "- `np.random.seed(1)` 和 `random.seed(1)` 都是用来设置随机数生成器的种子，如果种子一样，随机数生成器生成的随机数也会一样\n",
    "- `np.random.seed(1)`仅影响`np.random`中的随机数生成函数，即只有`np.random.rand()`才会受这个种子影响\n",
    "- `random.seed(1)`仅影响`random`中的随机数生成函数，即只有`random.rand()`才会受这个种子影响\n",
    "- `np.random`提供了更丰富的随机数生成功能，尤其适用于科学计算和处理多维数组。它支持生成多种概率分布的随机数（如正态分布、均匀分布、二项分布等），还可以生成随机矩阵\n",
    "- `random`提供了基本的随机数生成工具，适合一般用途。支持生成单个随机数或在列表中随机选择元素等\n",
    "\n",
    "##### torch.nn.Embedding(2708, 1433)\n",
    "- 创建一个嵌入层，用于将离散的索引映射成一个多维的张量，第一个参数是索引的范围，第二个参数是输出张量的维度\n",
    "- 该层的输入可以是任意维度的LongTensor张量，但张量的每个元素必须位于[0, 2707]这个范围内；输出中，每个元素对应一个1433维度的Float张量\n",
    "- 嵌入层只有一个权重矩阵，该权重矩阵是一个二维的矩阵（或者称为张量），其中每一行对应于一个特定的类别或节点的嵌入向量\n",
    "- 嵌入层的主要功能是将离散的输入（如类别ID或节点ID）映射到一个连续的向量空间，因此权重矩阵的行数与类别的数量一致，列数则是嵌入向量的维度\n",
    "- 嵌入层没有截距项，因为它只是简单地将输入映射到一个预定义的向量\n",
    "- 这条命令不仅创建了一个嵌入层，还自动初始化了它的权重矩阵，初始化为随机状态（符合某种分布）\n",
    "> 在神经网络中，创建一个层通常意味着创建一个或多个权重矩阵和（在某些情况下）截距项（也称为偏置项）。不过，具体是否会创建权重矩阵和截距项，取决于你创建的层的类型\n",
    "\n",
    "##### torch.FloatTensor\n",
    "- 创建一个元素为float32类型的张量\n",
    "- arg1：数据，可选，列表/元组/numpy数组/张量，torch.FloatTensor([1.0, 2.0, 3.0])\n",
    "- arg2：形状，可选，如果没有数据的情况下，可以直接输入形状，torch.FloatTensor(2, 3)\n",
    "\n",
    "##### torch.nn.Parameter\n",
    "- 将张量注册为模型参数，当将一个张量转换为 nn.Parameter 并将其赋值给 nn.Module 的一个属性时，这个张量会自动被视为模型的参数，PyTorch 的优化器会自动识别这些参数，如果requires_grad=true，会在反向传播时更新它们的值，否则，这个参数将不会被更新\n",
    "- 是一个特殊的张量，默认情况下它会被视为模型的可学习参数，在反向传播时会计算它的梯度并更新\n",
    "- arg1：张量，必选\n",
    "- arg2：requires_grad，默认为true，\n",
    "\n",
    "##### enc1(nodes).t()\n",
    "- 在 PyTorch 中，当你创建一个 nn.Module 的子类（如 Encoder），并调用该对象时，实际上会调用该类的 forward 方法。这是 PyTorch 的惯例：当你调用 enc1(nodes) 时，等价于 enc1.forward(nodes)\n",
    "\n",
    "##### lambda\n",
    "- 匿名函数 lambda 参数列表 : 表达式\n",
    "- 没有名字，用于定义短小的函数\n",
    "- MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)这个类的第一个参数需要传入一个函数，因此这里用了一个匿名函数\n",
    "\n",
    "##### np.random.permutation(num_nodes)\n",
    "- 生成一个随机排列的序列\n",
    "- 当你传递一个整数 num_nodes 作为参数时，np.random.permutation(num_nodes) 会生成一个从 0 到 num_nodes-1 的整数序列，并对该序列进行随机排列\n",
    "- 当传入一个列表作为参数时，相当于打乱该列表\n",
    "\n",
    "##### filter\n",
    "- filter 函数是 Python 的一个内置函数，用于从一个可迭代对象（如列表、元组等）中过滤出符合条件的元素，并返回一个迭代器\n",
    "- filter(function, iterable)，对于迭代器中的每个元素都执行function函数，function函数返回真（例如非零数值、非空字符串、非空容器等），才能留下\n",
    "\n",
    "##### lambda p : p.requires_grad\n",
    "- 匿名函数，返回元素的requires_grad值\n",
    "\n",
    "##### graphsage.parameters()\n",
    "- 可以递归识别出所有模型的参数（包括嵌套使用的模型），只有nn.Parameter类型的对象才会被识别，因此在写代码的时候，需要被训练的参数需要定义为nn.Parameter类型\n",
    "\n",
    "##### torch.optim.SGD\n",
    "- 是 PyTorch 中的一个优化器类，用于实现 随机梯度下降（SGD, Stochastic Gradient Descent） 算法\n",
    "- arg1：需要优化的参数列表或可迭代对象\n",
    "- arg2：lr，学习率，也就是步长\n",
    "\n",
    "##### time.time()\n",
    "- time.time() 是 Python time 模块中的一个函数，用于获取当前时间的时间戳。这个时间戳表示的是从1970年1月1日（Unix纪元）到当前时刻的秒数，是一个浮点数\n",
    "\n",
    "##### random.shuffle\n",
    "- 是 Python random 模块中的一个函数，用于将一个列表中的元素随机打乱顺序。它直接在原列表上进行操作，改变列表中元素的排列顺序，而不会返回一个新的列表\n",
    "\n",
    "##### optimizer.zero_grad()\n",
    "- 清空现有的参数梯度\n",
    "- 在使用 PyTorch 进行训练时，通常会遵循以下步骤：\n",
    "  - 前向传播：将输入数据传递给模型，计算出预测值\n",
    "  - 计算损失：使用损失函数计算预测值与真实标签之间的误差\n",
    "  - 反向传播：调用 loss.backward() 计算损失函数相对于模型参数的梯度。PyTorch 会自动对每个参数计算其梯度，并累加到参数的 .grad 属性中\n",
    "  - 参数更新：使用优化器（例如 torch.optim.SGD）更新模型的参数。优化器会根据计算出的梯度和指定的学习率来调整参数的值\n",
    "- 在 PyTorch 中，反向传播时计算的梯度会累加到已经存在的梯度中，而不会自动替换现有的梯度。因此，如果你不在每次迭代开始前清除现有的梯度，那么这些梯度会一直累加，这可能导致模型参数更新不正确，训练过程出现问题。\n",
    "\n",
    "##### labels[np.array(batch_nodes)]\n",
    "- label是numpy数组，为了避免不必要的错误，索引最好也用np.array\n",
    "\n",
    "##### loss.backward()\n",
    "- 计算梯度：loss.backward() 用于执行反向传播（backpropagation），即计算损失函数 loss 对模型参数的梯度。这些梯度被存储在每个参数的 .grad 属性中\n",
    "\n",
    "##### optimizer.step()\n",
    "- optimizer.step() 用于根据先前计算的梯度更新模型的参数。优化器使用这些梯度信息以及指定的学习率（和其他可能的超参数，如动量）来调整模型的权重，从而使损失函数最小化\n",
    "\n",
    "##### f1_score\n",
    "- 是一个评估分类模型性能的指标，特别适用于处理不平衡数据集的情况\n",
    "- arg1：真实的标签，通常是一个数组或列表，包含每个样本的实际类别\n",
    "- arg2：预测的标签，通常是一个数组或列表，包含每个样本的实际类别\n",
    "- arg3：average，定义计算 F1 分数的方式，对于多类分类问题尤为重要；'micro': 计算全局的 F1 分数，将多类问题视作二类问题（对每个类别的贡献均等考虑）\n",
    "\n",
    "##### val_output.data.numpy().argmax(axis=1)\n",
    "- val_output.data老版本的pytorch访问张量值的方式\n",
    "- .numpy()将其转换成numpy数组\n",
    "- argmax表示返回指定维度上的最大值索引，因为预测输出为一个概率分布的矩阵，每一行为一个预测结果；axis=1，会在每一行的多个列中找到最大值的索引，这个索引对应的是模型对该样本的预测类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4f3f8-66b6-477d-82a2-25bfe117d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cora():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_nodes = 2708\n",
    "    # feat_data是一个(num_nodes, num_feats)的np数组，索引为node_index；\n",
    "    # labels是一个(num_nodes,1)的np数组，索引为node_index，值为label_index；\n",
    "    # adj_list是一个<node_index, set>的字典，存储每个节点的邻接节点\n",
    "    feat_data, labels, adj_lists = load_cora()\n",
    "    # 定义一个嵌入层，输入为索引，输出为每个索引对应的特征向量\n",
    "    features = nn.Embedding(2708, 1433)\n",
    "    # 初始特征值，h_v^0\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "    # features.cuda()\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "            base_model=enc1, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 5\n",
    "    enc2.num_samples = 5\n",
    "\n",
    "    graphsage = SupervisedGraphSage(7, enc2)\n",
    "#   graphsage.cuda()\n",
    "    \n",
    "    # mini_batch\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "    times = []\n",
    "    for batch in range(100):\n",
    "        batch_nodes = train[:256]\n",
    "        random.shuffle(train)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, \n",
    "                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "        print batch, loss.data[0]\n",
    "\n",
    "    val_output = graphsage.forward(val) \n",
    "    print \"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\")\n",
    "    print \"Average batch time:\", np.mean(times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "graphsage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
